# Task 02.3: Background Aggregation Service

**Phase:** 02 - Backend - F001: Public Contract Aggregation
**Parent Plan:** [MVP Implementation Plan Overview](../00-mvp-implementation-plan-overview.md)
**Last Updated:** 2025-06-06

## 1. Objective

To implement a background service/task that periodically fetches public contract data from ESI for relevant regions, processes it, and stores/updates it in the local database using the defined data models and ESI client.

## 2. Relevant Specifications

*   `/design/specifications/design-spec.md` (Sections: Background Processing, Data Aggregation)
*   `/design/features/F001-Public-Contract-Aggregation-Display.md` (Data freshness requirements)
*   Task 02.1: ESI API Client (Public Endpoints)
*   Task 02.2: Data Models for F001

## 3. Key Implementation Steps

*   [ ] **Choose and Integrate Background Task Framework:**
    *   **Decision:** We will use `APScheduler` with `AsyncIOScheduler` for its simplicity and direct integration with `asyncio`.
    *   **Integration:** The scheduler instance must be managed via FastAPI's lifecycle events. It should be initialized and started on `startup` and gracefully shut down on `shutdown`. This follows the pattern established in Phase 1 for managing shared resources.
    *   **Dependency:** Add `apscheduler` to the project's dependencies using PDM.
*   [ ] **Aggregation Logic:**
    *   **Concurrency Lock:** Implement a locking mechanism (e.g., a specific key in the Valkey cache) to ensure only one instance of the aggregation job runs at a time. The job should check for the lock at the start and exit immediately if it's already held. It must release the lock upon completion or failure.
    *   **Service Class:** Create a dedicated service class (e.g., `ContractAggregationService`) to encapsulate the aggregation logic. This class should be instantiated with dependencies like the database session and ESI client.
    *   **Configuration:** The service will depend on new Pydantic settings: a list of region IDs to scan and a list of ship group IDs to identify ship contracts. These should be added to `config.py` and loaded from environment variables.
    *   **Region Iteration:** The service should iterate over the configured list of region IDs.
    *   **Data Fetching & Processing:** For each region, use the ESI client to fetch contracts and their items. Implement robust error handling for ESI API calls, logging errors without crashing the entire job.
    *   **Data Transformation:** Transform the raw ESI data into the SQLAlchemy models (`Contract`, `ContractItem`, `EsiTypeCache`).
    *   **Implement Business Logic for Ship Contracts:**
        *   **Definition:** A "ship contract" is defined as an `item_exchange` contract containing exactly one packaged item (`is_singleton=True`) that belongs to a known ship group.
        *   **Logic:**
            1.  During aggregation, after fetching contract items, check if a contract meets the definition.
            2.  To verify if an item is a ship, check its `group_id` from the `EsiTypeCache`.
            3.  Maintain a configurable list of ship `group_id`s in Pydantic settings (e.g., Frigates, Destroyers, Cruisers, Battleships, etc.).
            4.  If the criteria are met, set the `Contract.is_ship_contract` flag to `True`. Then, copy all relevant details from the primary ship `ContractItem` to the `Contract` record: `ship_type_id`, `quantity`, `is_blueprint_copy`, `runs`, `material_efficiency`, and `time_efficiency`.
            5.  Additionally, if the total number of items in the contract is greater than one, set the `Contract.contains_additional_items` flag to `True`.
    *   **Pre-mortem Note (Simplification Challenge):** The logic for identifying ship contracts and denormalizing specific fields, while necessary, adds complexity to the main aggregation loop. If this logic becomes significantly more intricate in the future, consider refactoring it into a separate, testable component or step in the processing pipeline.
    *   **Database Upsert:** Use an efficient `upsert` strategy to insert new records and update existing ones. Batch operations to reduce database round-trips.
    *   **AI Prompt:** "Generate a Python class `ContractAggregationService` that orchestrates the public contract aggregation. It should include:
        1. An `__init__` method to accept a DB session, ESI client, and cache client.
        2. A `run_aggregation` async method that:
                      a. Implements a cache-based lock to prevent concurrent runs. **Crucially, if the lock cannot be checked due to a cache connection error, the method must log a critical error and exit immediately without running the aggregation (failing 'closed' or 'safe').**
           b. Fetches a list of region IDs from settings.
           c. Iterates through regions, logging progress and handling ESI errors gracefully. **Pre-mortem Note (Poison Pill Data):** Within each region, wrap the processing of individual contracts (or small batches of contracts) in `try...except` blocks. If a specific contract's data causes an unrecoverable error during transformation or DB preparation, log the problematic `contract_id` and details, skip that contract, and continue with the next in the region. This prevents one bad data item from halting all processing for a region.
           d. For each region, fetches contracts and items, using the ESI client's ETag caching.
           e. Manages the `EsiTypeCache` by upserting newly discovered `type_id`s.
            f. **Implements a data refresh policy:** At the start of each run, it queries for a small number of `EsiTypeCache` records where `last_refreshed_at` is older than a configured threshold (e.g., 30 days). It then re-fetches and updates these records from ESI. If a specific type consistently fails to refresh (e.g., due to ESI errors for that type_id), log the error and update its `last_refreshed_at` timestamp to prevent it from being immediately re-selected in the next run, thus avoiding repeated futile calls. This ensures local data doesn't become stale while being resilient to problematic individual types.
           g. Upserts contract and item data to the database in batches.
           h. Logs a summary at the end (total contracts processed, types refreshed, duration, errors).
           i. Ensures the lock is released in a `finally` block. **Pre-mortem Note:** The lock release itself should be in a nested `try...except` to catch potential cache errors during release, logging them critically but ensuring the main aggregation error (if any) is not masked. Log the success or failure of the lock release explicitly."
*   [ ] **Scheduling:**
    *   Configure the chosen scheduler (`APScheduler`) to run the aggregation logic at regular intervals (e.g., every 15-30 minutes, configurable).
    *   Ensure the scheduler is started when the FastAPI application starts.
    *   **AI Prompt:** "Show how to integrate `APScheduler` with a FastAPI application to run a specific async function periodically."
*   [ ] **Database Interaction:**
    *   The background task cannot use the standard FastAPI `Depends` system. A new, independent database session must be created for the duration of the job run. Create a utility that provides a session and ensures it is closed correctly, similar to the `get_db` context manager.
    *   Use SQLAlchemy 2.0 style `merge()` or provider-specific `INSERT ... ON CONFLICT` statements for efficient upserts.
    *   Batch all database writes (e.g., using `session.add_all()`) to minimize I/O.
*   [ ] **Structured Logging:**
    *   **CRITICAL:** Ensure `logging.basicConfig()` is called in `main.py` as discovered in Phase 1, otherwise logs from the scheduler may not be visible.
    *   Log the start and end of each aggregation run with a unique run ID for traceability.
    *   Log key statistics for each run: duration, number of regions processed, contracts fetched/updated, items processed, and errors encountered.
    *   Log cache hits/misses for ESI ETag checks to monitor efficiency.
    *   Log any error with a full stack trace, but allow the service to continue to the next region where possible. **Pre-mortem Note (Logging Volume):** Be mindful of logging levels within tight loops (e.g., per-contract processing). Use `DEBUG` for highly verbose logs and ensure production log levels are typically `INFO` or `WARNING` to manage log volume and cost.

## 4. AI Implementation Guidance

*   Focus on resilience: the aggregator should handle ESI errors or temporary unavailability without crashing. If persistent database connection or commit errors occur, the aggregation run should be aborted, log a critical error, and ensure the concurrency lock is released.
*   Make region list and schedule interval configurable (via Pydantic settings).
*   Ensure efficient database updates (upserts) to avoid duplicates and keep data current. **Pre-mortem Note:** Carefully tune batch sizes for database upserts. Too small increases round-trips; too large can cause memory issues or long transaction times. Monitor DB performance during aggregation runs.
*   If using `APScheduler` with `asyncio`, ensure jobs are scheduled correctly in the event loop.
    *   **Pre-mortem Note (Scheduler Health):** Implement basic monitoring or logging for `APScheduler` itself. Log job submission, success, failure, and execution times. Consider setting up alerts if jobs fail repeatedly or if execution times deviate significantly, as this could indicate underlying issues (DB slowness, ESI issues, bugs).

## 5. Definition of Done

*   A background task mechanism (`APScheduler` or similar) is integrated.
*   Aggregation logic to fetch, process, and store contract data for specified regions is implemented.
*   The aggregation task is scheduled to run periodically.
*   Data is correctly upserted into the `Contract` and `ContractItem` tables.
*   Adequate logging is in place for monitoring the aggregation process.
*   All new files and code changes are committed to version control.

## 6. Challenges & Resolutions

*   **Potential Challenge 1: Concurrent Job Runs**
    *   **Symptom:** If an aggregation run takes longer than the scheduled interval, a new job starts before the previous one finishes, causing race conditions, duplicated data, and excessive load.
    *   **Proactive Resolution:** Implement a robust locking mechanism using the cache (Valkey). The job must acquire a lock before starting and release it in a `finally` block to guarantee it's always released, even on error.

*   **Potential Challenge 2: Scheduler in Async Environment**
    *   **Symptom:** Scheduled jobs do not run, or they block the main application's event loop.
    *   **Proactive Resolution:** Use `APScheduler`'s `AsyncIOScheduler`. Ensure it is started and shut down correctly within FastAPI's `startup` and `shutdown` events. All scheduled jobs must be `async` functions.

*   **Potential Challenge 3: Scalability with Many Regions**
    *   **Symptom:** As the number of configured regions increases, the sequential processing of each region makes the total aggregation job duration excessively long.
    *   **Proactive Resolution (Future Consideration):** For MVP, sequential processing is acceptable. If job duration becomes an issue, explore parallel processing of regions (e.g., using `asyncio.gather` to run ESI calls and DB operations for multiple regions concurrently), ensuring that database connection pooling and ESI rate limits are respected.

## 7. Cross-Cutting Concerns Review

This section documents how the five key cross-cutting concerns were addressed during the completion of this task. Refer to the primary specification documents for detailed guidance:
*   Security: `/design/specifications/security-spec.md`
*   Observability: `/design/specifications/observability-spec.md`
*   Testing: `/design/specifications/test-spec.md`
*   Accessibility: `/design/specifications/accessibility-spec.md`
*   Internationalization (i18n): `/design/specifications/i18n-spec.md`

### 7.1. Security
*   [ ] **Secure Design:** (e.g., threat modeling, principle of least privilege)
*   [ ] **Input Validation:** (e.g., validating all external inputs)
*   [ ] **Output Encoding:** (e.g., preventing XSS)
*   [ ] **Authentication/Authorization:** (e.g., ensuring proper checks)
*   [ ] **Secrets Management:** (e.g., secure storage and access)
*   [ ] **Dependency Management:** (e.g., checking for vulnerable libraries)
*   **Notes:** (Detail specific actions taken or rationale for no action, especially if a category is not applicable to this task.)

### 7.2. Observability
*   [x] **Structured Logging:** All logs should be structured (e.g., JSON) to be machine-readable.
*   [x] **Key Events Logged:** The service will log the start and end of each run, the region being processed, and a final summary of contracts/items processed.
*   [x] **Error Logging:** All exceptions during the process will be logged with stack traces.
*   [ ] **Correlation IDs:** A unique ID will be generated for each aggregation run and included in all related log messages.
*   [x] **Metrics:** Key metrics will be logged, including run duration, number of ESI calls, number of DB upserts, and cache hit/miss ratio.
*   **Notes:** As this is a background task, detailed logging is the primary mechanism for observability. Remember the `logging.basicConfig()` setup is essential.

### 7.3. Testing
*   [x] **Unit Tests:** Unit test the core transformation logic within the `ContractAggregationService`—given sample ESI data, does it produce the correct SQLAlchemy model instances? The ESI client and DB session should be mocked.
*   [x] **Integration Tests:** Write integration tests that:
    1.  Mock the `APScheduler` so you can trigger the job manually instead of waiting.
    2.  Use `pytest-httpx` to mock the ESI API endpoints, returning realistic success and error responses.
    3.  Use a test database to verify that the service correctly upserts data based on the mocked API responses.
    4.  Test the locking mechanism by attempting to trigger the job twice and asserting the second one does not run.
*   [ ] **Test Coverage:** Aim for high coverage on the service class logic.
*   [x] **Test Data Management:** Use static JSON files as fixtures for ESI API responses.
*   **Notes:** Testing a scheduled background task requires mocking the time/scheduling component and external dependencies (HTTP, DB).

### 7.4. Accessibility (A11y)
*(Primarily for UI-related tasks, but consider CLI/API accessibility where relevant)*
*   [ ] **Semantic HTML/Structure:** (e.g., using appropriate tags for meaning)
*   [ ] **ARIA Attributes:** (e.g., for dynamic content or custom controls)
*   [ ] **Keyboard Navigability:** (e.g., all interactive elements reachable and operable via keyboard)
*   [ ] **Color Contrast:** (e.g., ensuring sufficient contrast for text and UI elements)
*   [ ] **Screen Reader Compatibility:** (e.g., testing with screen readers)
*   [ ] **Alternative Text for Images:** (e.g., providing descriptive alt text)
*   **Notes:** (Detail specific actions taken or rationale for no action, especially if not UI-related.)

### 7.5. Internationalization (I18n)
*(Primarily for UI-related tasks, but consider for any user-facing text including logs/error messages)*
*   [ ] **Text Abstraction:** (e.g., using translation keys instead of hardcoded strings)
*   [ ] **Locale-Specific Formatting:** (e.g., for dates, numbers, currencies)
*   [ ] **UI Layout Adaptability:** (e.g., for text expansion in different languages)
*   [ ] **Character Encoding:** (e.g., using UTF-8)
*   **Notes:** (Detail specific actions taken or rationale for no action, especially if not UI-related.)

---
<!-- This section should be placed before any final "Task Completion Checklist" or similar concluding remarks. -->
